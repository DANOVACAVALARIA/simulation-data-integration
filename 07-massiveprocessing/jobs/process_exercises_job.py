import json
import sys
import io
import struct
from datetime import datetime
from typing import List, Dict, Optional, Tuple

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import udf, col, explode, lit, size, when
from pyspark.sql.types import (
    StringType, StructType, StructField, IntegerType, 
    FloatType, DoubleType, LongType, ArrayType
)

# ========== CONFIGURA√á√ïES ==========
class Config:
    SOURCE_BUCKET = 'bronze'
    WAREHOUSE_BUCKET = 'warehouse'
    AWS_S3_ENDPOINT = 'http://minio-storage:9000'
    AWS_ACCESS_KEY = 'admin'
    AWS_SECRET_KEY = 'password'
    WAREHOUSE = f"s3a://{WAREHOUSE_BUCKET}/"
    NESSIE_URI = "http://nessie-app:19120/api/v1"
    MAX_TEXT_LENGTH = 1_000_000
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB

# ========== UTILIT√ÅRIOS DE LOG ==========
def log_info(message: str):
    print(f"[PROCESS EXERCISES] ‚ÑπÔ∏è {message}")

def log_success(message: str):
    print(f"[PROCESS EXERCISES] ‚úÖ {message}")

def log_error(message: str):
    print(f"[PROCESS EXERCISES] ‚ùå {message}")

# ========== PROCESSAMENTO ==========
def extract_pdf_text(pdf_bytes: bytes) -> str:
    """Extrai texto de PDF"""
    if not pdf_bytes or len(pdf_bytes) == 0:
        return None
    
    if len(pdf_bytes) > Config.MAX_FILE_SIZE:
        return "[AVISO: Arquivo muito grande]"
    
    try:
        import pdfplumber
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            max_pages = min(len(pdf.pages), 50)
            text = '\n'.join([pdf.pages[i].extract_text() or "" for i in range(max_pages)])
            clean_text = ' '.join(text.split())
            
            if len(clean_text) > Config.MAX_TEXT_LENGTH:
                clean_text = clean_text[:Config.MAX_TEXT_LENGTH] + "...[TRUNCADO]"
            
            return clean_text if clean_text else "[VAZIO]"
            
    except ImportError:
        try:
            import PyPDF2
            reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
            max_pages = min(len(reader.pages), 50)
            text = '\n'.join([reader.pages[i].extract_text() or "" for i in range(max_pages)])
            clean_text = ' '.join(text.split())
            
            if len(clean_text) > Config.MAX_TEXT_LENGTH:
                clean_text = clean_text[:Config.MAX_TEXT_LENGTH] + "...[TRUNCADO]"
                
            return clean_text if clean_text else "[VAZIO]"
        except Exception as e:
            return f"[ERRO PyPDF2: {str(e)[:100]}]"
    except Exception as e:
        return f"[ERRO: {str(e)[:100]}]"

def extract_dis_data(dis_bytes: bytes) -> List[Dict]:
    """Extrai dados de arquivo DIS"""
    if not dis_bytes or len(dis_bytes) == 0:
        return []
    
    try:
        from opendis.PduFactory import createPdu
        
        data = []
        buffer = dis_bytes
        offset = 0
        max_records = 10000
        
        while offset < len(buffer) - 12 and len(data) < max_records:
            try:
                pdu = createPdu(buffer[offset:])
                
                if pdu and hasattr(pdu, 'pduType') and pdu.pduType == 1:
                    data.append({
                        "EntityID": int(getattr(pdu.entityID, 'entityID', 0)),
                        "ForceID": int(getattr(pdu, 'forceId', 0)),
                        "Kind": int(getattr(pdu.entityType, 'entityKind', 0)),
                        "Latitude": float(getattr(pdu.entityLocation, 'x', 0.0)),
                        "Longitude": float(getattr(pdu.entityLocation, 'y', 0.0)),
                        "Altitude": float(getattr(pdu.entityLocation, 'z', 0.0)),
                        "Timestamp": int(getattr(pdu, 'timestamp', 0))
                    })
                
                offset += getattr(pdu, 'length', 12)
                
            except Exception:
                offset += 12
                continue
        
        return data
    
    except Exception as e:
        log_error(f"Erro ao extrair dados DIS: {str(e)}")
        return []

def decode_protobuf_messages(protobuf_bytes: bytes) -> List[Dict]:
    """Decodifica mensagens Protocol Buffer"""
    if not protobuf_bytes or len(protobuf_bytes) == 0:
        return []

    try:
        from simulation_client_pb2 import SimToClient
        
        messages = []
        buffer = protobuf_bytes
        index = 0
        max_messages = 10000
        
        while index < len(buffer) and len(messages) < max_messages:
            try:
                if index + 4 > len(buffer):
                    break
                
                length = struct.unpack(">I", buffer[index:index+4])[0]
                index += 4
                
                if length <= 0 or length > 1000000:
                    break
                
                if index + length > len(buffer):
                    break
                    
                msg_data = buffer[index:index+length]
                index += length
                
                msg = SimToClient()
                msg.ParseFromString(msg_data)
                
                message_record = {
                    "context": int(msg.context) if hasattr(msg, 'context') else None,
                    "client_id": int(msg.client_id) if hasattr(msg, 'client_id') else None,
                    "knowledge_id": None,
                    "knowledge_group_id": None,
                    "latitude": None,
                    "longitude": None,
                    "altitude": None,
                    "timestamp": None
                }
                
                if (hasattr(msg, 'message') and 
                    msg.message.HasField('unit_knowledge_update')):
                    
                    u = msg.message.unit_knowledge_update
                    message_record.update({
                        "knowledge_id": int(u.knowledge.id) if hasattr(u, 'knowledge') and hasattr(u.knowledge, 'id') else None,
                        "knowledge_group_id": int(u.knowledge_group.id) if hasattr(u, 'knowledge_group') and hasattr(u.knowledge_group, 'id') else None,
                        "latitude": float(u.position.latitude) if hasattr(u, 'position') and hasattr(u.position, 'latitude') else None,
                        "longitude": float(u.position.longitude) if hasattr(u, 'position') and hasattr(u.position, 'longitude') else None,
                        "altitude": float(u.height_f) if hasattr(u, 'height_f') else None,
                        "timestamp": int(u.pertinence) if hasattr(u, 'pertinence') else None
                    })
                
                messages.append(message_record)
                
            except Exception:
                index += 1
                continue
        
        return messages
        
    except Exception as e:
        log_error(f"Erro ao extrair dados protobuf: {str(e)}")
        return []

# ========== UDFs ==========
extract_pdf_udf = udf(extract_pdf_text, StringType())
extract_dis_udf = udf(extract_dis_data, ArrayType(StructType([
    StructField("EntityID", IntegerType()),
    StructField("ForceID", IntegerType()), 
    StructField("Kind", IntegerType()),
    StructField("Latitude", FloatType()),
    StructField("Longitude", FloatType()),
    StructField("Altitude", FloatType()),
    StructField("Timestamp", IntegerType())
])))

extract_protobuf_udf = udf(decode_protobuf_messages, ArrayType(StructType([
    StructField("context", IntegerType()),
    StructField("client_id", IntegerType()),
    StructField("knowledge_id", IntegerType()),
    StructField("knowledge_group_id", IntegerType()),
    StructField("latitude", DoubleType()),
    StructField("longitude", DoubleType()),
    StructField("altitude", DoubleType()),
    StructField("timestamp", LongType())
])))

# ========== PROCESSADOR ==========
class ExerciseProcessor:
    def __init__(self, spark: SparkSession):
        self.spark = spark

    def process_documents(self, exercise: Dict) -> Optional[DataFrame]:
        """Processa documentos PDF"""
        log_info("Processando documentos PDF")
        
        documents = exercise['metadata'].get('documents', [])
        if not documents:
            return None
        
        pdf_files = []
        doc_metadata = {}
        
        for doc in documents:
            if (doc.get('path') and doc.get('name', '').lower().endswith('.pdf')):
                pdf_path = f"s3a://{Config.SOURCE_BUCKET}/{doc['path']}"
                pdf_files.append(pdf_path)
                doc_metadata[pdf_path] = doc.get('name', doc['path'].split('/')[-1])
        
        if not pdf_files:
            return None
        
        all_records = []
        
        try:
            file_paths = ','.join(pdf_files)
            binary_df = self.spark.read.format("binaryFile").load(file_paths)
            
            processed_df = binary_df.withColumn("extracted_text", extract_pdf_udf(col("content"))) \
                .select(
                    lit(exercise['id']).alias("exercise_id"),
                    col("path").alias("source_file"),
                    col("extracted_text"),
                    when(col("extracted_text").startswith("[ERRO"), "ERROR").otherwise("SUCCESS").alias("status"),
                    lit(str(datetime.now())).alias("processed_at")
                ).filter(col("extracted_text").isNotNull())
            
            batch_results = processed_df.collect()
            
            for row in batch_results:
                file_path = row['source_file']
                document_name = doc_metadata.get(file_path, file_path.split('/')[-1])
                
                record = {
                    'exercise_id': row['exercise_id'],
                    'document_name': document_name,
                    'extracted_text': row['extracted_text'],
                    'status': row['status'],
                    'processed_at': row['processed_at']
                }
                all_records.append(record)
            
        except Exception as e:
            log_error(f"Erro no processamento de documentos: {str(e)}")
            return None
        
        if not all_records:
            return None
        
        result_df = self.spark.createDataFrame(all_records)
        log_success(f"PDFs processados: {result_df.count()} registros")
        
        return result_df

    def process_simulations(self, exercise: Dict) -> Tuple[Optional[DataFrame], str]:
        """Processa simula√ß√µes"""
        log_info("Processando simula√ß√µes")
        
        simulations = exercise['metadata'].get('simulations', [])
        if not simulations:
            return None, "NONE"
        
        sim_config = simulations[0] if isinstance(simulations, list) else simulations
        if not sim_config.get('path'):
            return None, "NONE"
        
        files, message_type = self._find_simulation_files(Config.SOURCE_BUCKET, sim_config['path'])
        
        if not files:
            return None, "NONE"
        
        try:
            if message_type == 'DIS':
                result_df = self._process_dis_files(files, exercise['id'])
                return result_df, message_type
            elif message_type == 'PROTO':
                result_df = self._process_protobuf_files(files, exercise['id'])
                return result_df, message_type
            else:
                raise Exception(f"Tipo de mensagem n√£o suportado: {message_type}")
                
        except Exception as e:
            log_error(f"Erro no processamento de simula√ß√µes: {str(e)}")
            raise

    def _process_dis_files(self, input_files: List[str], exercise_id: str) -> DataFrame:
        """Processa arquivos DIS - CORRE√á√ÉO SIMPLES"""
        try:
            log_info(f"üîß Processando {len(input_files)} arquivos DIS")
            
            # CORRE√á√ÉO: Use um padr√£o que o Spark aceite ou processe individualmente
            if len(input_files) == 1:
                # Se √© apenas um arquivo, use diretamente
                file_path = input_files[0]
                binary_df = self.spark.read.format("binaryFile").load(file_path)
            else:
                # Se s√£o m√∫ltiplos arquivos, processe um por vez e una os DataFrames
                dfs = []
                for file_path in input_files:
                    filename = file_path.split('/')[-1]
                    log_info(f"üìÑ Carregando {filename}")
                    df = self.spark.read.format("binaryFile").load(file_path)
                    dfs.append(df)
                
                # Una todos os DataFrames
                binary_df = dfs[0]
                for df in dfs[1:]:
                    binary_df = binary_df.union(df)
            
            # Resto do processamento continua igual
            processed_df = binary_df.withColumn("pdu_data", extract_dis_udf(col("content"))) \
                .filter(col("pdu_data").isNotNull() & (size(col("pdu_data")) > 0)) \
                .select(
                    lit(exercise_id).alias("exercise_id"),
                    explode(col("pdu_data")).alias("pdu"),
                    lit(str(datetime.now())).alias("processed_at")
                ).select(
                    "exercise_id",
                    col("pdu.EntityID").alias("entity_id"),
                    col("pdu.ForceID").alias("force_id"),
                    col("pdu.Kind").alias("kind"),
                    col("pdu.Latitude").alias("latitude"),
                    col("pdu.Longitude").alias("longitude"),
                    col("pdu.Altitude").alias("altitude"),
                    col("pdu.Timestamp").alias("timestamp"),
                    "processed_at"
                )
            
            count = processed_df.count()
            if count > 0:
                log_success(f"üéâ Arquivos DIS processados: {count} registros")
                return processed_df
            else:
                raise Exception("Nenhum registro DIS extra√≠do")
                
        except Exception as e:
            log_error(f"üí• Erro ao processar arquivos DIS: {str(e)}")
            raise
    # def _process_dis_files(self, input_files: List[str], exercise_id: str) -> DataFrame:
    #     """Processa arquivos DIS"""
    #     try:
    #         file_paths = ','.join(input_files)
    #         binary_df = self.spark.read.format("binaryFile").load(file_paths)
            
    #         processed_df = binary_df.withColumn("pdu_data", extract_dis_udf(col("content"))) \
    #             .filter(col("pdu_data").isNotNull() & (size(col("pdu_data")) > 0)) \
    #             .select(
    #                 lit(exercise_id).alias("exercise_id"),
    #                 explode(col("pdu_data")).alias("pdu"),
    #                 lit(str(datetime.now())).alias("processed_at")
    #             ).select(
    #                 "exercise_id",
    #                 col("pdu.EntityID").alias("entity_id"),
    #                 col("pdu.ForceID").alias("force_id"),
    #                 col("pdu.Kind").alias("kind"),
    #                 col("pdu.Latitude").alias("latitude"),
    #                 col("pdu.Longitude").alias("longitude"),
    #                 col("pdu.Altitude").alias("altitude"),
    #                 col("pdu.Timestamp").alias("timestamp"),
    #                 "processed_at"
    #             )
            
    #         count = processed_df.count()
    #         if count > 0:
    #             log_success(f"Arquivos DIS processados: {count} registros")
    #             return processed_df
    #         else:
    #             raise Exception("Nenhum registro DIS extra√≠do")
                
    #     except Exception as e:
    #         log_error(f"Erro ao processar arquivos DIS: {str(e)}")
    #         raise

    def _process_protobuf_files(self, input_files: List[str], exercise_id: str) -> DataFrame:
        """Processa arquivos Protobuf"""
        all_records = []
        
        for file_path in input_files:
            try:
                binary_data = self.spark.sparkContext.binaryFiles(file_path).collect()[0][1]
                
                if len(binary_data) == 0:
                    continue
                
                messages = decode_protobuf_messages(binary_data)
                
                for msg in messages:
                    record = {
                        'exercise_id': exercise_id,
                        'context': msg.get('context'),
                        'client_id': msg.get('client_id'),
                        'knowledge_id': msg.get('knowledge_id'),
                        'knowledge_group_id': msg.get('knowledge_group_id'),
                        'latitude': msg.get('latitude'),
                        'longitude': msg.get('longitude'),
                        'altitude': msg.get('altitude'),
                        'timestamp': msg.get('timestamp'),
                        'status': 'SUCCESS',
                        'processed_at': str(datetime.now())
                    }
                    all_records.append(record)
                
            except Exception as e:
                log_error(f"Erro ao processar arquivo {file_path}: {str(e)}")
                continue
        
        if not all_records:
            raise Exception("Nenhuma mensagem protobuf extra√≠da")
        
        result_df = self.spark.createDataFrame(all_records)
        total_count = result_df.count()
        log_success(f"Protobuf processado: {total_count} registros")
        
        return result_df

    def save_table(self, df: DataFrame, table_name: str):
        """Salva DataFrame como tabela Iceberg"""
        try:
            count = df.count()
            log_info(f"Salvando {count} registros na tabela {table_name}")
            
            if count > 0:
                table_exists = self._table_exists(table_name)
                mode = "append" if table_exists else "overwrite"
                
                df.write.format("iceberg").mode(mode).saveAsTable(table_name)
                log_success(f"Tabela {table_name} salva com sucesso")
            else:
                log_error(f"DataFrame vazio para tabela {table_name}")
                
        except Exception as e:
            log_error(f"Erro ao salvar tabela {table_name}: {str(e)}")
            raise
    # def _find_simulation_files(self, bucket: str, sim_path: str) -> Tuple[List[str], str]:
    #     """Busca arquivos de simula√ß√£o com logs detalhados para debug"""
        
    #     log_info(f"üîç INICIANDO BUSCA DE ARQUIVOS")
    #     log_info(f"   Bucket: {bucket}")
    #     log_info(f"   Caminho: {sim_path}")
        
    #     try:
    #         # Normaliza o caminho
    #         clean_path = sim_path.rstrip('/')
    #         full_path = f"s3a://{bucket}/{clean_path}/"
            
    #         log_info(f"   Caminho completo: {full_path}")
    #         log_info(f"   Padr√£o de busca: {full_path}**")
            
    #         # Tentativa de busca recursiva
    #         log_info("üìÇ Executando busca recursiva...")
    #         try:
    #             all_files = self.spark.read.option("recursiveFileLookup", "true") \
    #                 .format("binaryFile").load(f"{full_path}**") \
    #                 .select("path", "length").collect()
                
    #             log_info(f"‚úÖ Busca executada com sucesso: {len(all_files)} arquivos encontrados")
                
    #         except Exception as search_error:
    #             log_error(f"‚ùå Erro na busca recursiva: {str(search_error)}")
    #             log_info("üîÑ Tentando busca alternativa...")
                
    #             # Busca alternativa sem recurs√£o
    #             try:
    #                 all_files = self.spark.read.format("binaryFile").load(f"{full_path}*") \
    #                     .select("path", "length").collect()
    #                 log_info(f"‚úÖ Busca alternativa: {len(all_files)} arquivos encontrados")
    #             except Exception as alt_error:
    #                 log_error(f"‚ùå Busca alternativa tamb√©m falhou: {str(alt_error)}")
    #                 raise Exception(f"N√£o foi poss√≠vel acessar o diret√≥rio: {str(search_error)}")

    #         # Lista todos os arquivos encontrados
    #         if len(all_files) == 0:
    #             log_error("üìÅ NENHUM arquivo encontrado no diret√≥rio")
    #             raise Exception(f"Diret√≥rio vazio ou inacess√≠vel: {full_path}")
            
    #         log_info("üìã ARQUIVOS ENCONTRADOS:")
    #         for i, row in enumerate(all_files):
    #             file_path = row.path
    #             file_size = row.length
    #             filename = file_path.split('/')[-1]
    #             log_info(f"   {i+1:2d}. {filename}")
    #             log_info(f"       Caminho: {file_path}")
    #             log_info(f"       Tamanho: {file_size:,} bytes")

    #         # Categoriza arquivos
    #         bin_files = []
    #         pb_files = []
    #         other_files = []
            
    #         log_info("üîç CATEGORIZANDO ARQUIVOS:")
            
    #         for row in all_files:
    #             file_path = row.path
    #             file_size = row.length
    #             filename = file_path.split('/')[-1].lower()
                
    #             # Verifica se o arquivo √© v√°lido
    #             is_valid = True
    #             status_msgs = []
                
    #             if file_size == 0:
    #                 is_valid = False
    #                 status_msgs.append("‚ùå Arquivo vazio")
    #             elif file_size > Config.MAX_FILE_SIZE:
    #                 is_valid = False
    #                 status_msgs.append(f"‚ùå Muito grande ({file_size:,} > {Config.MAX_FILE_SIZE:,})")
    #             else:
    #                 status_msgs.append("‚úÖ Tamanho v√°lido")
                
    #             # Categoriza por extens√£o
    #             if filename.endswith('.bin'):
    #                 if is_valid:
    #                     bin_files.append(file_path)
    #                     status_msgs.append("üéØ Arquivo DIS (.bin)")
    #                 else:
    #                     status_msgs.append("‚ö†Ô∏è  Arquivo .bin inv√°lido")
                        
    #             elif filename.endswith('.pb'):
    #                 if is_valid:
    #                     pb_files.append(file_path)
    #                     status_msgs.append("üéØ Arquivo Protobuf (.pb)")
    #                 else:
    #                     status_msgs.append("‚ö†Ô∏è  Arquivo .pb inv√°lido")
                        
    #             else:
    #                 other_files.append(file_path)
    #                 status_msgs.append("‚ÑπÔ∏è  Outro tipo")
                
    #             log_info(f"   üìÑ {filename}: {' | '.join(status_msgs)}")

    #         # Relat√≥rio de categoriza√ß√£o
    #         log_info("üìä RELAT√ìRIO DE CATEGORIZA√á√ÉO:")
    #         log_info(f"   üéØ Arquivos .bin v√°lidos: {len(bin_files)}")
    #         log_info(f"   üéØ Arquivos .pb v√°lidos: {len(pb_files)}")
    #         log_info(f"   üìÑ Outros arquivos: {len(other_files)}")

    #         # Lista os arquivos v√°lidos encontrados
    #         if bin_files:
    #             log_info("üéØ ARQUIVOS DIS (.bin) V√ÅLIDOS:")
    #             for i, bf in enumerate(bin_files):
    #                 filename = bf.split('/')[-1]
    #                 log_info(f"   {i+1}. {filename}")
    #                 log_info(f"      {bf}")
            
    #         if pb_files:
    #             log_info("üéØ ARQUIVOS PROTOBUF (.pb) V√ÅLIDOS:")
    #             for i, pf in enumerate(pb_files):
    #                 filename = pf.split('/')[-1]
    #                 log_info(f"   {i+1}. {filename}")
    #                 log_info(f"      {pf}")

    #         # Testa acessibilidade dos arquivos encontrados
    #         if bin_files:
    #             log_info("üî¨ TESTANDO ACESSIBILIDADE DOS ARQUIVOS .bin:")
    #             accessible_bin_files = []
                
    #             for i, bin_file in enumerate(bin_files):
    #                 filename = bin_file.split('/')[-1]
    #                 try:
    #                     # Tenta ler os metadados do arquivo
    #                     test_read = self.spark.read.format("binaryFile").load(bin_file) \
    #                         .select("path", "length").collect()
                        
    #                     if test_read and len(test_read) > 0 and test_read[0].length > 0:
    #                         accessible_bin_files.append(bin_file)
    #                         log_info(f"   ‚úÖ {i+1}. {filename} - ACESS√çVEL ({test_read[0].length:,} bytes)")
    #                     else:
    #                         log_error(f"   ‚ùå {i+1}. {filename} - SEM DADOS")
                            
    #                 except Exception as access_error:
    #                     log_error(f"   ‚ùå {i+1}. {filename} - ERRO: {str(access_error)}")
                
    #             if accessible_bin_files:
    #                 log_success(f"üéâ SUCESSO: {len(accessible_bin_files)} arquivos DIS acess√≠veis encontrados!")
    #                 return accessible_bin_files, "DIS"
    #             else:
    #                 log_error("üí• FALHA: Nenhum arquivo .bin acess√≠vel!")
                    
    #         if pb_files:
    #             log_info("üî¨ Arquivos .pb encontrados, usando Protobuf")
    #             return pb_files, "PROTO"
            
    #         # Se chegou aqui, n√£o encontrou arquivos v√°lidos
    #         log_error("üí• FALHA FINAL: Nenhum arquivo de simula√ß√£o v√°lido encontrado!")
    #         log_info("üîç RESUMO DO QUE FOI ENCONTRADO:")
    #         log_info(f"   Total de arquivos: {len(all_files)}")
    #         log_info(f"   Arquivos .bin: {len([f for f in all_files if f.path.lower().endswith('.bin')])}")
    #         log_info(f"   Arquivos .pb: {len([f for f in all_files if f.path.lower().endswith('.pb')])}")
    #         log_info(f"   Outros arquivos: {len(other_files)}")
            
    #         # Lista alguns exemplos de outros arquivos para debug
    #         if other_files:
    #             log_info("üìÑ EXEMPLOS DE OUTROS ARQUIVOS:")
    #             for i, of in enumerate(other_files[:5]):
    #                 filename = of.split('/')[-1]
    #                 log_info(f"   {i+1}. {filename}")
            
    #         raise Exception(f"Nenhum arquivo de simula√ß√£o (.bin/.pb) v√°lido encontrado em {full_path}")
            
    #     except Exception as e:
    #         log_error(f"üí• ERRO CR√çTICO na busca de arquivos: {str(e)}")
    #         raise


    def _find_simulation_files(self, bucket: str, sim_path: str) -> Tuple[List[str], str]:
        """Busca arquivos de simula√ß√£o"""
        try:
            full_path = f"s3a://{bucket}/{sim_path.rstrip('/')}/"
            
            all_files = self.spark.read.option("recursiveFileLookup", "true") \
                .format("binaryFile").load(f"{full_path}**") \
                .select("path", "length").collect()

            bin_files = []
            pb_files = []
            
            for row in all_files:
                file_path = row.path
                
                if row.length > Config.MAX_FILE_SIZE:
                    continue
                    
                if file_path.lower().endswith('.bin'):
                    bin_files.append(file_path)
                elif file_path.lower().endswith('.pb'):
                    pb_files.append(file_path)

            if bin_files:
                return bin_files, "DIS"
            elif pb_files:
                return pb_files, "PROTO"
            else:
                raise Exception("Nenhum arquivo de simula√ß√£o encontrado")
            
        except Exception as e:
            log_error(f"Erro ao buscar arquivos: {str(e)}")
            raise

    def _table_exists(self, table_name: str) -> bool:
        """Verifica se tabela existe"""
        try:
            self.spark.sql(f"DESCRIBE TABLE {table_name}")
            return True
        except Exception:
            return False

def create_spark_session() -> SparkSession:
    """Cria sess√£o Spark"""
    log_info("Criando sess√£o Spark")
    
    try:
        configs = {
            # S3
            "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
            "spark.hadoop.fs.s3a.access.key": Config.AWS_ACCESS_KEY,
            "spark.hadoop.fs.s3a.secret.key": Config.AWS_SECRET_KEY,
            "spark.hadoop.fs.s3a.endpoint": Config.AWS_S3_ENDPOINT,
            "spark.hadoop.fs.s3a.path.style.access": "true",
            "spark.hadoop.fs.s3a.connection.ssl.enabled": "false",
            
            # Iceberg/Nessie
            "spark.sql.catalog.nessie": "org.apache.iceberg.spark.SparkCatalog",
            "spark.sql.catalog.nessie.catalog-impl": "org.apache.iceberg.nessie.NessieCatalog",
            "spark.sql.catalog.nessie.uri": Config.NESSIE_URI,
            "spark.sql.catalog.nessie.ref": "main",
            "spark.sql.catalog.nessie.warehouse": Config.WAREHOUSE,
            "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions",
        }
        
        builder = SparkSession.builder.appName("ProcessExercises")
        for key, value in configs.items():
            builder = builder.config(key, value)
        
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")
        
        log_success("Sess√£o Spark criada com sucesso")
        return spark
        
    except Exception as e:
        log_error(f"Erro ao criar sess√£o Spark: {str(e)}")
        raise

def main():
    """Fun√ß√£o principal"""
    log_info("Iniciando processamento de exerc√≠cios")
    
    if len(sys.argv) < 2:
        log_error("Par√¢metros insuficientes")
        sys.exit(1)
    
    spark = None
    try:
        selected_exercise = json.loads(sys.argv[1])
        log_info(f"Exerc√≠cio: {selected_exercise.get('id', 'ID n√£o encontrado')}")
        
        spark = create_spark_session()
        processor = ExerciseProcessor(spark)
        
        # Criar namespace
        spark.sql("CREATE NAMESPACE IF NOT EXISTS nessie.exercises")
        log_success("Namespace criado/verificado")
        
        # Processar documentos
        documents_df = processor.process_documents(selected_exercise)
        if documents_df:
            processor.save_table(documents_df, "nessie.exercises.pdf_documents")
        
        # Processar simula√ß√µes
        has_simulations = bool(selected_exercise['metadata'].get('simulations', []))
        simulation_processed = False
        
        if has_simulations:
            simulations_df, message_type = processor.process_simulations(selected_exercise)
            
            if simulations_df:
                if message_type == "DIS":
                    processor.save_table(simulations_df, "nessie.exercises.dis_messages")
                elif message_type == "PROTO":
                    processor.save_table(simulations_df, "nessie.exercises.protobuf_messages")
                simulation_processed = True
            else:
                raise Exception("Falha no processamento de simula√ß√µes")
        else:
            simulation_processed = True
        
        if has_simulations and not simulation_processed:
            raise Exception("Falha cr√≠tica no processamento de simula√ß√µes")
        
        log_success("Processamento conclu√≠do com sucesso!")
        
    except Exception as e:
        log_error(f"Erro cr√≠tico: {str(e)}")
        sys.exit(1)
        
    finally:
        if spark:
            spark.stop()

if __name__ == "__main__":
    main()